{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#setting figure size\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "import google_sheets_api as sheet #connection to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI Librarys \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllSheets():\n",
    "    sheet_aapl = sheet.AAPL_data.get_all_values() \n",
    "    sheet_amzn = sheet.AMZN_data.get_all_values()\n",
    "    sheet_csco = sheet.CSCO_data.get_all_values()\n",
    "    sheet_fb = sheet.FB_data.get_all_values()\n",
    "    sheet_googl = sheet.GOOGL_data.get_all_values()\n",
    "    sheet_IBM = sheet.IBM_data.get_all_values()\n",
    "    sheet_intc = sheet.INTC_data.get_all_values()\n",
    "    sheet_msft = sheet.MSFT_data.get_all_values()\n",
    "    sheet_orcl = sheet.ORCL_data.get_all_values()\n",
    "    sheet_qcom = sheet.QCOM_data.get_all_values()\n",
    "    sheet_tsla = sheet.TSLA_data.get_all_values()\n",
    "    sheet_vz = sheet.VZ_data.get_all_values()\n",
    "\n",
    "    sheet_aapl = sheet_aapl[1:] #remove column names title\n",
    "    sheet_amzn = sheet_amzn[1:] #remove column names title\n",
    "    sheet_csco = sheet_csco[1:] #remove column names title\n",
    "    sheet_fb = sheet_fb[1:] #remove column names title\n",
    "    sheet_googl = sheet_googl[1:] #remove column names title\n",
    "    sheet_intc = sheet_intc[1:] #remove column names title\n",
    "    sheet_msft = sheet_msft[1:] #remove column names title\n",
    "    sheet_IBM = sheet_IBM[1:] #remove column names title\n",
    "    sheet_orcl = sheet_orcl[1:] #remove column names title\n",
    "    sheet_qcom = sheet_qcom[1:] #remove column names title\n",
    "    sheet_tsla = sheet_tsla[1:] #remove column names title\n",
    "    sheet_vz = sheet_vz[1:] #remove column names title\n",
    "\n",
    "\n",
    "    \n",
    "    AAPL_data = pd.DataFrame(sheet_aapl, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    AMZN_data = pd.DataFrame(sheet_amzn, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    CSCO_data = pd.DataFrame(sheet_csco, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    FB_data = pd.DataFrame(sheet_fb, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    GOOGL_data = pd.DataFrame(sheet_googl, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    INTC_data = pd.DataFrame(sheet_intc, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    MSFT_data = pd.DataFrame(sheet_msft, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    IBM_data = pd.DataFrame(sheet_IBM, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    ORCL_data = pd.DataFrame(sheet_orcl, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    QCOM_data = pd.DataFrame(sheet_qcom, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    TSLA_data = pd.DataFrame(sheet_tsla, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "    VZ_data = pd.DataFrame(sheet_vz, columns = ['timestamp','symbol','tradingDay','open','high','low','close','volume']) \n",
    "\n",
    "\n",
    "    return [AAPL_data, AMZN_data, CSCO_data, FB_data, GOOGL_data, INTC_data, MSFT_data, IBM_data, ORCL_data, QCOM_data, TSLA_data, VZ_data]\n",
    "\n",
    "    \n",
    "all_stock_data = getAllSheets()  ##THIS VAR HOLDS ALL OF THE STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL = all_stock_data[0] #AAPL\n",
    "df_AMZN = all_stock_data[1] #AMZN\n",
    "df_CSCO = all_stock_data[2] #CSCO\n",
    "df_FB = all_stock_data[3] #FB\n",
    "df_GOOGL = all_stock_data[4] #GOOGL\n",
    "df_INTC = all_stock_data[5] #INTC\n",
    "df_MSFT = all_stock_data[6] #MSFT\n",
    "df_IBM = all_stock_data[7] #IBM\n",
    "df_ORCL = all_stock_data[8] #ORCL\n",
    "df_QCOM = all_stock_data[9] #QCOM\n",
    "df_TLSA = all_stock_data[10] #TSLA\n",
    "df_VZ = all_stock_data[11] #VZ\n",
    "\n",
    "all_stock_dataframes = [df_AAPL, df_AMZN, df_CSCO, df_FB, df_GOOGL, df_INTC, df_MSFT, df_IBM, df_ORCL, df_QCOM, df_TLSA, df_VZ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for df in all_stock_dataframes:  \n",
    "    df['open'] = df['open'].astype(float)\n",
    "    df['high'] = df['high'].astype(float)\n",
    "    df['low'] = df['low'].astype(float)\n",
    "    df['close'] = df['close'].astype(float)\n",
    "    df['volume'] = df['volume'].astype(int)\n",
    "    #for index, row in df.iterrows():    \n",
    "        #if row['timestamp'].endswith('-05:00'):\n",
    "    #df['timestamp'].iloc[index] = row['timestamp'][:-6]\n",
    "               \n",
    "    df.index = df['timestamp']\n",
    "    print(count, \"of \", len(all_stock_dataframes), \"stock dataframes successfully reformatted\")\n",
    "    count+=1\n",
    "\n",
    "print(\"All data successfully reformatted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_ticker(ticker):\n",
    "    if (ticker == 'AAPL'):\n",
    "        return df_AAPL\n",
    "    if (ticker == 'AMZN'):\n",
    "        return df_AMZN\n",
    "    if (ticker == 'CSCO'):\n",
    "        return df_CSCO\n",
    "    if (ticker == 'FB'):\n",
    "        return df_FB\n",
    "    if (ticker == 'GOOGL'):\n",
    "        return df_GOOGL\n",
    "    if (ticker == 'INTC'):\n",
    "        return df_INTC\n",
    "    if (ticker == 'MSFT'):\n",
    "        return df_MSFT\n",
    "    if (ticker == 'IBM'):\n",
    "        return df_IBM\n",
    "    if (ticker == 'ORCL'):\n",
    "        return df_ORCL\n",
    "    if (ticker == 'QCOM'):\n",
    "        return df_QCOM\n",
    "    if (ticker == 'TLSA'):\n",
    "        return df_TLSA\n",
    "    if (ticker == 'VZ'):\n",
    "        return df_VZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_day(df):\n",
    "    day_splits = [0]\n",
    "    #print(df['tradingDay'])\n",
    "    iterator_day = df['tradingDay'].iloc[0]\n",
    "    #print(iterator_day)\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        counter += 1\n",
    "        #print(row['tradingDay'])\n",
    "        if (iterator_day == row['tradingDay']):\n",
    "            continue\n",
    "        else:\n",
    "            day_splits.append(counter)\n",
    "            iterator_day = row['tradingDay']\n",
    "    return day_splits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_split(current_stock_df):\n",
    "    total_rows = current_stock_df.count()\n",
    "   # training_data_split = int(total_rows['close']*0.945) #80% of the data becomes train data\n",
    "    training_data_split = int(total_rows['close']) #80% of the data becomes train data\n",
    "    print(training_data_split)\n",
    "    #training_data_split = 717\n",
    "    return training_data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_day(df):\n",
    "    day_splits = [0]\n",
    "    #print(df['tradingDay'])\n",
    "    iterator_day = df['tradingDay'].iloc[0]\n",
    "    #print(iterator_day)\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        counter += 1\n",
    "        #print(row['tradingDay'])\n",
    "        if (iterator_day == row['tradingDay']):\n",
    "            continue\n",
    "        else:\n",
    "            day_splits.append(counter)\n",
    "            iterator_day = row['tradingDay']\n",
    "    return day_splits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulated_data(symbol, data_points):\n",
    "    current_stock_df = determine_ticker(symbol)\n",
    "    daySplits = split_by_day(current_stock_df)\n",
    "    total_rows = len(current_stock_df.index)\n",
    "    daySplits.append(total_rows)\n",
    "    print(\"DaySplit: \", daySplits)\n",
    "    \n",
    "    #creating dataframe\n",
    "    data = current_stock_df.sort_index(ascending=True, axis=0)\n",
    "    new_data = pd.DataFrame(index=range(0,len(current_stock_df)),columns=['timestamp', data_points[0], data_points[1],data_points[2],data_points[3],data_points[4]])\n",
    "\n",
    "    \n",
    "    #creating a new_data DF for manipulation\n",
    "    for i in range(0,len(data)):\n",
    "        new_data['timestamp'][i] = data['timestamp'][i]\n",
    "        new_data[data_points[0]][i] = data[data_points[0]][i]\n",
    "        new_data[data_points[1]][i] = data[data_points[1]][i]\n",
    "        new_data[data_points[2]][i] = data[data_points[2]][i]\n",
    "        new_data[data_points[3]][i] = data[data_points[3]][i]\n",
    "        new_data[data_points[4]][i] = data[data_points[4]][i]\n",
    "\n",
    "        \n",
    "    #setting index to timestamp \n",
    "    new_data.index = new_data.timestamp\n",
    "    new_data.drop('timestamp', axis=1, inplace=True)\n",
    "    \n",
    "    return current_stock_df, new_data, daySplits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(formatted_data, sensitivity, data_points, training_data_split, day_split, num_predication_points):\n",
    "    lookback = 75\n",
    "    totalInputs = len(data_points)\n",
    "    #creating train and test sets\n",
    "    dataset = formatted_data.values \n",
    "        \n",
    "    len_for_graphing = len(formatted_data)\n",
    "    \n",
    "    train = dataset[0:training_data_split,:] #TRAINGING 80%\n",
    "   # valid = dataset[training_data_split:,:] #DATA POINTS TO BE PREDICTED 20%\n",
    "    \n",
    "\n",
    "    valid = np.empty((num_predication_points,totalInputs), int)\n",
    "    #print(valid)\n",
    "\n",
    "    #converting dataset into x_train and y_train\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))   #NORMALIZARION between O and 1 of the data\n",
    "    scaled_data = scaler.fit_transform(dataset)   #FITS OUR DATA SET TO THE NORMALIZATION BETWEEN ONE AND ZERO\n",
    "      \n",
    "    #Spliting the data into X train and Y Train\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(lookback,len(train)):\n",
    "        x_train.append(scaled_data[i-lookback:i,:])\n",
    "        y_train.append(scaled_data[i,1])\n",
    "\n",
    "    \n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],totalInputs))  \n",
    " \n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential() \n",
    "    model.add(LSTM(units=lookback, return_sequences=True, input_shape=(x_train.shape[1],totalInputs))) #2 input\n",
    "    model.add(LSTM(units=30, return_sequences=True))\n",
    "    model.add(LSTM(units=30))\n",
    "    model.add(Dense(units=1))\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=sensitivity, batch_size=32)  #EPOCHS determine sensitivity\n",
    "    model.summary()\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    \n",
    "    #predicting values, using past lookback from the train data\n",
    "    inputs = formatted_data[len(formatted_data) - len(valid) - lookback: len(formatted_data) - len(valid)].values\n",
    "    inputs = inputs.reshape(-1,totalInputs) \n",
    "    \n",
    "    to_be_predicted = len(valid)\n",
    "    \n",
    "    print(\"len(formatted_data):\", len(formatted_data))\n",
    "    print(\"len(valid):\", len(valid))\n",
    "    print(\"len(formatted_data) - len(valid) - lookback:\", len(formatted_data) - len(valid) - lookback)\n",
    "    \n",
    "    #print(inputs)\n",
    "   # print(inputs.shape)\n",
    "    \n",
    "    inputs  = scaler.transform(inputs)\n",
    "    \n",
    "    X_test = []\n",
    "    overall_predictions = []\n",
    "    X_test.append(inputs)\n",
    "    for x in range(to_be_predicted):\n",
    "        \n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "\n",
    "        predictons = model.predict(X_test) #PREDICTIONS \n",
    "        formated_preds = np.zeros((predictons.shape[0],predictons.shape[1]+(totalInputs-1)))\n",
    "        formated_preds[:,:-1] = predictons\n",
    "        predictons = scaler.inverse_transform(formated_preds) #DE NORMALIZARION\n",
    "\n",
    "        updated_xtests = []\n",
    "        updated_xtests = np.concatenate((X_test[-1], [predictons[-1]]), axis=0)\n",
    "        \n",
    "        updated_xtests = updated_xtests[1:]\n",
    "        updated_xtests = np.reshape(updated_xtests, (1,updated_xtests.shape[0],updated_xtests.shape[1]))\n",
    "\n",
    "        X_test = np.concatenate((X_test, updated_xtests), axis=0)\n",
    "        #X_test = np.reshape(X_test, (1,X_test.shape[0],X_test.shape[1]))\n",
    "       # print(\"X_test: \", X_test.shape)\n",
    "        overall_predictions.append(predictons[-1][0])\n",
    "\n",
    "    \n",
    "       \n",
    "    overall_predictions = np.array(overall_predictions)\n",
    "    #overall_predictions = np.reshape(overall_predictions, (overall_predictions.shape[0], 1))\n",
    " #   print(\"overall_predictions: \", overall_predictions)\n",
    "    \n",
    "#    print(overall_predictions.shape)\n",
    "    #For Plotting\n",
    "    \n",
    " #   print(\"formatted_data: \", formatted_data)\n",
    " #   print(\"formatted_data type: \", type(formatted_data))\n",
    "    \n",
    "    new_formatted_data = pd.DataFrame(valid, columns = ['open','close','low','high', 'volume'])\n",
    "    formatted_data = formatted_data.append(new_formatted_data)\n",
    "    formatted_data.index = range(len(formatted_data))\n",
    "    \n",
    "    #print(formatted_data)\n",
    "    \n",
    "    train = formatted_data[:training_data_split]\n",
    "    valid = formatted_data[len_for_graphing:len_for_graphing+40]\n",
    "\n",
    "  #  print(\"valid len:\", len(valid))\n",
    "  #  print(valid)\n",
    "    \n",
    "    valid['Predictions'] = overall_predictions\n",
    "   # valid['Percent Error'] = valid.apply(lambda row: (abs(row.iloc[0]-row.iloc[5])/row.iloc[0]*100), axis=1)\n",
    "   # print(valid)\n",
    "\n",
    "    # Find local peaks\n",
    "    valid['min'] = valid['Predictions'][(valid['Predictions'].shift(1) > valid['Predictions']) & (valid['Predictions'].shift(-1) > valid['Predictions'])]\n",
    "    valid['max'] = valid['Predictions'][(valid['Predictions'].shift(1) < valid['Predictions']) & (valid['Predictions'].shift(-1) < valid['Predictions'])]\n",
    "    #print(\"The avg Percent error:\", round(abs(valid['Percent Error']).mean(),3))\n",
    "    \n",
    "    \n",
    "    #Plot 1 - all data\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(len(day_split)):\n",
    "        plt.axvline(x=day_split[i],linestyle='--',color='g')\n",
    "    \n",
    "    plt.plot(train[data_points[0]], 'b')\n",
    "  #  plt.plot(valid[data_points[0]], 'c')\n",
    "    plt.plot(valid['Predictions'], color='orange')\n",
    "    \n",
    "    plt.xlabel('timestamp')\n",
    "    plt.ylabel(data_points[0])\n",
    "    plt.title(symbol)\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plot 2 - just some data\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(len(day_split)):\n",
    "        if (i > 12):\n",
    "            plt.axvline(x=day_split[i],linestyle='--',color='g')\n",
    "    \n",
    "    plt.plot(train[data_points[0]].iloc[521:], 'b')\n",
    "  #  plt.plot(valid[data_points[0]], 'c')\n",
    "    plt.plot(valid['Predictions'], color='orange')\n",
    "    \n",
    "    plt.xlabel('timestamp')\n",
    "    plt.ylabel(data_points[0])\n",
    "    plt.title(symbol)\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #Graph 3 - just predictions\n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(symbol + \" \" + str(current_stock_df['tradingDay'].iloc[-1]))\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "   # plt.plot(valid[data_points[0]], 'c')\n",
    "    plt.plot(valid['Predictions'], color='orange')\n",
    "    \n",
    "    plt.plot(valid['min'], '-p', color='gray',\n",
    "         markersize=15, linewidth=0,\n",
    "         markerfacecolor='white',\n",
    "         markeredgecolor='blue',\n",
    "         markeredgewidth=2)\n",
    "    \n",
    "    plt.plot(valid['max'], '-p', color='gray',\n",
    "         markersize=15, linewidth=0,\n",
    "         markerfacecolor='white',\n",
    "         markeredgecolor='black',\n",
    "         markeredgewidth=2)\n",
    "    \n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #Graph 3 Errors\n",
    " #   plt.figure(figsize=(15,10))\n",
    "    \n",
    " #   plt.xlabel('Datetime')\n",
    " #   plt.ylabel('Error (%)')\n",
    " #   plt.title(\"Percent Error Over the course of prediction\")\n",
    " #   plt.xticks(rotation=90)\n",
    "\n",
    "  #  plt.plot(valid['Percent Error'], color='red')\n",
    "    \n",
    " #   plt.legend(prop={'size': 15})\n",
    " #   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'FB'\n",
    "data_points = ['open','close','low','high','volume']\n",
    "sensitivity = 25\n",
    "\n",
    "num_predication_points = 40\n",
    "\n",
    "current_stock_df, new_data, day_split = manipulated_data(symbol, data_points) #symbol and what to use\n",
    "training_data_split = set_data_split(current_stock_df)\n",
    "\n",
    "create_LSTM_model(new_data, sensitivity, data_points, training_data_split, day_split, num_predication_points) #formatted data, and epoch sensitivity, what to use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'FB'\n",
    "data_points = ['open','close','low','high','volume']\n",
    "sensitivity = 25\n",
    "\n",
    "num_predication_points = 40\n",
    "\n",
    "current_stock_df, new_data, day_split = manipulated_data(symbol, data_points) #symbol and what to use\n",
    "training_data_split = set_data_split(current_stock_df)\n",
    "\n",
    "create_LSTM_model(new_data, sensitivity, data_points, training_data_split, day_split, num_predication_points) #formatted data, and epoch sensitivity, what to use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'FB'\n",
    "data_points = ['open','close','low','high','volume']\n",
    "sensitivity = 25\n",
    "\n",
    "num_predication_points = 40\n",
    "\n",
    "current_stock_df, new_data, day_split = manipulated_data(symbol, data_points) #symbol and what to use\n",
    "training_data_split = set_data_split(current_stock_df)\n",
    "\n",
    "create_LSTM_model(new_data, sensitivity, data_points, training_data_split, day_split, num_predication_points) #formatted data, and epoch sensitivity, what to use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'FB'\n",
    "data_points = ['open','close','low','high','volume']\n",
    "sensitivity = 25\n",
    "\n",
    "num_predication_points = 40\n",
    "\n",
    "current_stock_df, new_data, day_split = manipulated_data(symbol, data_points) #symbol and what to use\n",
    "training_data_split = set_data_split(current_stock_df)\n",
    "\n",
    "create_LSTM_model(new_data, sensitivity, data_points, training_data_split, day_split, num_predication_points) #formatted data, and epoch sensitivity, what to use \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
